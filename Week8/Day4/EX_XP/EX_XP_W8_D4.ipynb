{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ Exercise 1: Calculating Required Sample Size\n",
        "You are planning an A/B test to evaluate the impact of a new email subject line on the open rate. Based on past data, you expect a small effect size of 0.3 (an increase from 20% to 23% in the open rate). You aim for an 80% chance (power = 0.8) of detecting this effect if it exists, with a 5% significance level (Î± = 0.05).\n",
        "\n",
        "Calculate the required sample size per group using Pythonâ€™s statsmodels library.\n",
        "What sample size is needed for each group to ensure your test is properly powered?\n",
        "\n"
      ],
      "metadata": {
        "id": "0LAJEg1Jb1nY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKok5V4jbzQS",
        "outputId": "5500849b-f7a5-43bf-ecc8-da7462ec6467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 2941\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.stats.power import NormalIndPower\n",
        "from statsmodels.stats.proportion import proportion_effectsize\n",
        "\n",
        "p1 = 0.20\n",
        "p2 = 0.23\n",
        "alpha = 0.05\n",
        "power = 0.80\n",
        "ratio = 1\n",
        "\n",
        "effect_size = proportion_effectsize(p1, p2)\n",
        "\n",
        "analysis = NormalIndPower()\n",
        "\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=ratio)\n",
        "\n",
        "sample_size = int(sample_size) + 1\n",
        "\n",
        "print(f\"Required sample size per group: {sample_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ Exercise 2: Understanding the Relationship Between Effect Size and Sample Size\n",
        "Using the same A/B test setup as in Exercise 1, you want to explore how changing the expected effect size impacts the required sample size.\n",
        "\n",
        "Calculate the required sample size for the following effect sizes: 0.2, 0.4, and 0.5, keeping the significance level and power the same.\n",
        "How does the sample size change as the effect size increases? Explain why this happens."
      ],
      "metadata": {
        "id": "UWhOeknqcCXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.power import NormalIndPower\n",
        "import numpy as np\n",
        "\n",
        "alpha = 0.05\n",
        "power = 0.80\n",
        "ratio = 1\n",
        "\n",
        "effect_sizes = [0.2, 0.4, 0.5]\n",
        "\n",
        "analysis = NormalIndPower()\n",
        "\n",
        "print(\"Calculating required sample sizes for different effect sizes:\\n\")\n",
        "print(\"Effect Size\\tRequired Sample Size per Group\")\n",
        "print(\"-----------\\t------------------------------\")\n",
        "\n",
        "for es in effect_sizes:\n",
        "    sample_size = analysis.solve_power(effect_size=es, power=power, alpha=alpha, ratio=ratio)\n",
        "    sample_size = int(np.ceil(sample_size))\n",
        "    print(f\"{es}\\t\\t{sample_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUxsMy9-cDQx",
        "outputId": "494cdb21-3430-43f5-dc14-0ebf76b440a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating required sample sizes for different effect sizes:\n",
            "\n",
            "Effect Size\tRequired Sample Size per Group\n",
            "-----------\t------------------------------\n",
            "0.2\t\t393\n",
            "0.4\t\t99\n",
            "0.5\t\t63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ Exercise 3: Exploring the Impact of Statistical Power\n",
        "Imagine you are conducting an A/B test where you expect a small effect size of 0.2. You initially plan for a power of 0.8 but wonder how increasing or decreasing the desired power level impacts the required sample size.\n",
        "\n",
        "Calculate the required sample size for power levels of 0.7, 0.8, and 0.9, keeping the effect size at 0.2 and significance level at 0.05.\n",
        "Question: How does the required sample size change with different levels of statistical power? Why is this understanding important when designing A/B tests?\n"
      ],
      "metadata": {
        "id": "Jlp354MMcDmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.power import NormalIndPower\n",
        "import numpy as np\n",
        "\n",
        "effect_size = 0.2\n",
        "alpha = 0.05\n",
        "power_levels = [0.7, 0.8, 0.9]\n",
        "ratio = 1\n",
        "\n",
        "analysis = NormalIndPower()\n",
        "\n",
        "print(\"Calculating required sample sizes for different power levels:\\n\")\n",
        "print(\"Power Level\\tRequired Sample Size per Group\")\n",
        "print(\"-----------\\t------------------------------\")\n",
        "\n",
        "for power in power_levels:\n",
        "    sample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=ratio)\n",
        "    sample_size = int(np.ceil(sample_size))\n",
        "    print(f\"{power}\\t\\t{sample_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qKVyXQQcH1W",
        "outputId": "4dc68a7f-9672-42a1-f519-44c2abbdb8fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating required sample sizes for different power levels:\n",
            "\n",
            "Power Level\tRequired Sample Size per Group\n",
            "-----------\t------------------------------\n",
            "0.7\t\t309\n",
            "0.8\t\t393\n",
            "0.9\t\t526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the desired statistical power increases, the required sample size per group also increases.\n",
        "Understanding this relationship helps in designing experiments that are adequately powered to detect meaningful effects. Ensure that the calculation is computed using the correct dimension."
      ],
      "metadata": {
        "id": "MmFG3db4d7qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ðŸŒŸ Exercise 4: Implementing Sequential Testing\n",
        "You are running an A/B test on two versions of a product page to increase the purchase rate. You plan to monitor the results weekly and stop the test early if one version shows a significant improvement.\n",
        "\n",
        "Define your stopping criteria.\n",
        "Decide how you would implement sequential testing in this scenario.\n",
        "At the end of week three, Version B has a p-value of 0.02. What would you do next?"
      ],
      "metadata": {
        "id": "oD7Il4HacIRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By defining clear stopping criteria and implementing sequential testing with appropriate statistical adjustments, possible to effectively balance the risks of Type I and Type II errors while allowing for early detection of significant effects. In this scenario, with Version B showing a p-value of 0.02 at the end of week three, which is below the adjusted significance level of 0.0221, should stop the test early and proceed with implementing Version B, as it has demonstrated a statistically significant improvement."
      ],
      "metadata": {
        "id": "ru0hSumZevd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ Exercise 5: Applying Bayesian A/B Testing\n",
        "Youâ€™re testing a new feature in your app, and you want to use a Bayesian approach. Initially, you believe the new feature has a 50% chance of improving user engagement. After collecting data, your analysis suggests a 65% probability that the new feature is better.\n",
        "\n",
        "Describe how you would set up your prior belief.\n",
        "After collecting data, how does the updated belief (posterior distribution) influence your decision?\n",
        "What would you do if the posterior probability was only 55%?"
      ],
      "metadata": {
        "id": "Cj2riue7cNZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Describe How You Would Set Up Your Prior Belief\n",
        "\n",
        "In Bayesian A/B testing, setting up your prior belief involves specifying your initial assumptions about the parameters before observing the data. Hereâ€™s how you can do it for this scenario:\n",
        "\n",
        "a. Define the Metric of Interest\n",
        "â€¢\tUser Engagement Rate: The proportion of users who engage with the app in a meaningful way (e.g., clicks, time spent, purchases).\n",
        "\n",
        "b. Choose Appropriate Probability Distributions\n",
        "\n",
        "Since weâ€™re dealing with proportions (rates between 0 and 1), the Beta distribution is suitable for modeling our beliefs about the engagement rates of both versions of the app.\n",
        "2. What Would You Do If the Posterior Probability Was Only 55%?\n",
        "\n",
        "A posterior probability of 55% indicates very weak evidence that the new feature is better.\n",
        "\n"
      ],
      "metadata": {
        "id": "OClnnrJgfU8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "ðŸŒŸ Exercise 6: Implementing Adaptive Experimentation\n",
        "Youâ€™re running a test with three different website layouts to increase user engagement. Initially, each layout gets 33% of the traffic. After the first week, Layout C shows higher engagement.\n",
        "\n",
        "Explain how you would adjust the traffic allocation after the first week.\n",
        "Describe how you would continue to adapt the experiment in the following weeks.\n",
        "What challenges might you face with adaptive experimentation, and how would you address them?"
      ],
      "metadata": {
        "id": "5NtkNBlGcQ_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Adjusting Traffic Allocation After the First Week:\n",
        "\n",
        "Since Layout C shows higher engagement, allocate more traffic to it while still testing Layouts A and B. For example, assign a higher percentage (e.g., 60-70%) to Layout C and distribute the remaining traffic between Layouts A and B to continue gathering data.\n",
        "\n",
        "2. Adapting the Experiment in the Following Weeks:\n",
        "\n",
        "Continue to monitor engagement metrics and adjust traffic allocation accordingly. Use adaptive algorithms like Multi-Armed Bandits (e.g., Thompson Sampling) to balance exploration (testing all layouts) and exploitation (favoring the best performer). Gradually increase traffic to the top-performing layout while ensuring others receive enough exposure for accurate assessment.\n",
        "\n",
        "3. Challenges and How to Address Them:\n",
        "\n",
        "â€¢\tStatistical Biases: Adaptive allocation can introduce bias, making it hard to compare layouts directly. Mitigate this by using statistical methods suited for adaptive experiments.\n",
        "\n",
        "â€¢\tInsufficient Data for Some Layouts: Less traffic to underperforming layouts may lead to unreliable metrics. Set minimum traffic thresholds to ensure sufficient data.\n",
        "\n",
        "â€¢\tUser Experience Consistency: Users might see different layouts on return visits. Address this by assigning users to a specific layout throughout the experiment (user consistency)."
      ],
      "metadata": {
        "id": "vN09LVFughNf"
      }
    }
  ]
}